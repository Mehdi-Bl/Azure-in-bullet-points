# Implement Knowledge Mining and Information Extraction Solutions

## Azure AI Search (Cognitive Search)

A powerful service for **indexing and searching data**, plus enriching it with AI skills (often called **Knowledge Mining** when using enrichment).

### Provisioning

Create an Azure AI Search service (specify tier, like basic or standard, which controls index storage and QPS). The service endpoint and admin key will be used to manage indexes.

## Indexes, Skillsets, and Indexers

### Index

**Schema** defining the fields and types that will be searchable or filterable. For example, an index "documents" might have fields:
- id
- title
- content
- contentVector (for embeddings), etc.

You define which fields are:
- **Searchable** (full-text search)
- **Filterable** (for OData filters)
- **Facetable**
- **Sortable**
- Support **suggestions/autocomplete**

### Data Source

Defines where data comes from (could be Azure Blob Storage, Azure SQL DB, Cosmos DB, etc.). It includes connection info (and possibly credentials).

### Skillset (Optional)

A skillset is a **pipeline of cognitive skills** to enrich data during indexing. Skills can be:
- **Prebuilt** (OCR, key phrase extraction, entity recognition, sentiment, translation, etc.)
- **Custom** (custom skill that calls an Azure Function for custom processing)

You define which fields of your input feed into which skills and store outputs in enrichment fields.

### Indexer

The indexer **ties the data source to the index**, running the skillset on each item:
- It will pull data from the source
- Apply the skillset (if any)
- Populate the index fields

Indexers can be run on a **schedule** or **on-demand**. They also handle tracking what's been indexed (using watermark or change detection if supported by the data source).

### Defining a Skillset

For example, if indexing PDFs from Blob storage, a skillset might do:
1. **OCR** on images (if PDFs have scanned images)
2. **Extract text** from PDF (built-in text extraction skill)
3. **Language detection** on the text
4. **Key phrase extraction** on the text
5. **Entity recognition** on the text

All these outputs can be attached as new fields (e.g., a collection field of key phrases, a collection of entities of various types).

You configure this via JSON (REST API or SDK). Each skill has:
- **Inputs** (which field to take)
- **Outputs** (a field in an intermediate enrichment store)

### The Knowledge Store

If enabled, you can have a **knowledge store** which dumps enrichment output to storage (as JSON, HTML, or table formats). This is useful if you want to analyze or use the intermediate data outside of search (e.g., use the enriched data for PowerBI or knowledge graph).

Knowledge store projections can output to:
- Azure Blob (files)
- Azure Table
- Azure SQL tables

With the enriched info.

### Running an Indexer

Once configured, run it. It will go through documents. Monitor indexer status for errors (e.g., if a document was too large or had parsing issues, those are reported).

## Semantic and Vector Search

### Querying the Index

Use the Search REST API or SDK:

**Simple search**: `?search=azure cognitive` which does a full-text search on all searchable fields.

**OData filters**: `$filter=category eq 'Technology' and publishedDate ge 2023-01-01`. Filters are done post-search to refine results.

**Query syntax**: Support Lucene query syntax if using searchMode=all or default. You can do fielded searches, phrase searches, etc.

### Semantic Search

Azure AI Search offers **semantic ranking**:
- If enabled on your service (requires Standard tier or above and additional config)
- You can call the query with `queryType=semantic`
- It will use the ML-based reranker to provide more relevant results and even **captions or answers** extracted from documents

Semantic search will give a **snippet that directly answers a question** if possible (like an extracted sentence) and highlights, which is very useful in Q&A scenarios.

It does come at extra cost and latency, but improves quality by using Bing-like AI reranking.

### Vector Search

Azure AI Search now supports **pure vector similarity search** in indexes that have vector fields. This is crucial for AI scenarios:

You can **store embeddings** of text (like using OpenAI's Embeddings for each document) in a vector field. Then at query time, provide an embedding of the user's query and do a **vector similarity search** to find relevant docs by concept, not just exact keywords.

This enables **semantic or concept-based retrieval** (e.g., find documents similar in meaning to the query).

**Hybrid search**: Azure Search supports combining vector and keyword search in one query. The results from both can be merged.

Under the hood, Azure Search uses algorithms (like HNSW) to index vectors for k-NN search efficiently. You just need to supply the vectors (either precomputed or via skillset with the built-in "Embed" skill calling Azure OpenAI or CogServices embedding model).

**Use cases**: Vector search is often used in **Retrieval Augmented Generation** with Azure OpenAI – you vector-search relevant passages and then feed them to GPT for answering a question.

### Facets and Aggregations

You can configure fields as **facetable**, then query with `facet=fieldName` to get counts of results by category (like how many results per category). Useful for building search refiners (e.g., "10 results in Technology, 5 in Finance").

### Suggesters and Autocomplete

You can define a **suggester** on an index that allows prefix searches for typeahead suggestions (e.g., typing "micros" might suggest "Microsoft" if in the index). Autocomplete is similar but goes through the index's terms.

### Custom Skills

If the built-in skills aren't enough, you can integrate **custom logic**:

For example, a custom skill might call an external API or perform a regex extraction. You register an **Azure Function** (or Logic App, etc.) and the indexer will call that for each document, passing input data (like a text) and expecting output (like a JSON fragment to merge).

Custom skills are defined in the skillset JSON with the endpoint and inputs/outputs mapping. Use this to, say, classify documents with your own ML model or do sentiment on a specific portion, etc.

### Security

If your data source is a database or blob storage with private access, you can set up data source authentication (like managed identity to Azure SQL, or connection strings). For blobs, often you use a storage account connection string or SAS.

If you need **per-document security trimming** (so that search results honor user's permissions), Azure Search supports that via enriched ACLs – you would have to index the ACLs of each item (like user/group IDs) and then filter at query time based on the user. This is complex to implement manually; no built-in AD integration except with SharePoint connector scenarios.

### Scaling

You can scale Azure Search by:
- **Replicas** (for QPS)
- **Partitions** (for index storage/sharding)

Use multiple replicas for high availability and to handle more concurrent queries. Choose the tier (Basic, S1, S2, S3) based on size and performance needs.

### Managing

Monitor indexer executions – they can be set to periodic runs for updating indexes (or use push API to push changes in real-time). Reindexing might be needed if schema changes or full data re-ingestion. Use Azure Search explorer or the portal to quickly test queries and view indexed data.

## Azure AI Document Intelligence

### Service Overview

**Document Intelligence** (formerly Form Recognizer) can extract structured data from documents. There are **prebuilt models** for common document types and **custom models** you can train for your specific forms.

### Prebuilt Models

These are out-of-the-box and do not require training:

Examples:
- **Invoice** model
- **Receipt** model
- **Business Card** model
- **Passport** model
- **ID Document** model

You submit an image/PDF of a receipt and the Receipt model returns fields like:
- Merchant Name
- Transaction Date
- Total
- Tax, etc., with values and confidence

Similarly, Invoice model extracts:
- Invoice number
- Due date
- Line items
- Amounts

There's also a **Layout model** that simply extracts text lines, tables, and selection marks (checkboxes) from any document without interpreting it.

**Read OCR** underlies much of this, extracting text.

Using prebuilt is as simple as calling the respective endpoint (or the general Analyze Document endpoint with the model ID like "prebuilt-invoice"). They are continuously improved by Microsoft.

### Custom Models

Two approaches historically:

#### Custom Form (Template) model

You label a set of training documents that have the **same structure** (like a specific form type). For each document, you:
1. Identify the fields on the form
2. Label them with the field name (e.g., on a structured form, draw bounding boxes on "Name:", "Address:", etc.)
3. After labeling ~5+ examples, train a model

This model learns the positions and patterns, so it works well for forms with consistent layout.

#### Custom Neural model

More recently, Form Recognizer supports training **without manual labeling** using deep learning by providing at least 5 samples; it can group and understand the structure (unsupervised training) – this might apply when forms are semi-structured or varied, using ML to figure out common fields.

### Document Intelligence Studio

You would use this tool to upload documents and label them or cluster them.

### Composed Model

After training multiple custom models (say one for invoices, one for purchase orders), you can **compose them** into a single model endpoint. Composition is basically ensemble: it will internally decide which sub-model to use for a given input.

This is helpful to have one model ID to call and it routes to the correct form model (e.g., if you have 5 types of forms).

### Testing & Accuracy

After training a custom model, the service provides accuracy metrics for each field (precision/recall). You should test with a few sample docs. If some fields are not being extracted or have errors, you may need to label more samples or ensure the training samples cover all variations.

### Using Models

To use a model (prebuilt or custom), call the **Analyze Document API** with model ID:
- For custom ones, model ID GUID is given after training
- The result includes fields with names, values, confidence, and bounding regions
- If table extraction is relevant, it will list tables and their cells

There's also a generic **Layout extraction** which is often step 1 in analysis.

For custom models, the names of fields will be as you labeled them. For prebuilt, they follow a predefined schema (check the docs for field names).

### Continuous Improvement

If you get more documents over time or new variations, you can further train (or use unlabeled learning to improve if supported). Keep **versioning** of your models.

### Limitations

Very complex layouts (e.g., free-form contracts) might not extract perfectly with either approach. In such cases, an approach might be to use **Azure AI Content Understanding** (below) or treat it as a knowledge mining problem with search + QA.

### Document Intelligence vs. Content Understanding

**Document Intelligence** is focused on forms and documents to pull structured fields. **Azure AI Content Understanding** is more about using GenAI to analyze content in documents, including unstructured docs, and perform tasks like summarization or classification.

## Azure AI Content Understanding

### Overview

A new **generative AI-based service** designed to analyze any type of content (text, documents, images, audio, video) and extract information or summaries from it. It's like an all-in-one AI pipeline that can do what a combination of Form Recognizer, Cognitive Search, and GPT might do, but packaged and simplified.

### Capabilities

#### Text Extraction (OCR)
It can extract text from images and PDFs (integrating OCR as needed).

#### Summarization
It can generate summaries of documents (leveraging generative models).

#### Classification
It can classify documents or content into categories.

#### Attribute Detection
Identify key attributes in text – this could be similar to entity extraction but with a generative approach (like finding if a document is an invoice, what its number is, even if unseen format).

#### Entity/Table/Image Extraction
It can pull out structured data like:
- Entities mentioned
- Tables within documents (probably converting them to JSON or CSV)
- Even images from documents (perhaps extracting embedded images)

**Essentially, Content Understanding tries to ground information and automate schema generation** – meaning it can figure out what data structure fits the content and output that.

### Multiple Modalities at Once

For example, feed it a document that contains text and images – it could:
- Extract text from the images
- Recognize the image content
- Combine with text analysis

### Usage

Content Understanding likely provides **pre-built recipes or templates** for common use cases (like "process a set of documents and get a summary and key points for each"). It's integrated in Foundry – in Foundry Tools, it would be a service you can call as part of a pipeline.

Possibly it has a straightforward API where you input a file and specify which tasks (like "extract all" or a subset).

### Example

If you provide an insurance claim document, Content Understanding could output:
- Summary of the claim
- Classification (claim type)
- Extract entities like Claim Number, Date, Customer Name
- Pull any tables (like list of damaged items)
- Any images (maybe photos of damage) inside it for separate analysis

It does so using generative AI under the hood to adapt to arbitrary content.

### Relation to Knowledge Mining

This service could **simplify knowledge mining** by not requiring manual setup of skillsets – it's an AI that understands the content holistically. Under Foundry, it works with Search and other tools (e.g., it can use Azure AI Search for its knowledge grounding).

It also can integrate with Speech and Translator if needed (since it's Foundry-based, likely one tool can call others).

### Generative Aspect

Unlike Form Recognizer which is fixed field extraction, Content Understanding might **dynamically decide what is important**. For instance, automating schema generation suggests it could look at a set of documents and figure out common fields among them, then output those fields for each doc.

That's powerful for quick automation, albeit perhaps less control on exact fields.

### Output and Integration

The output might be delivered as JSON with various sections (summary, entities, etc.). As a developer, you might use it to:
- Feed directly into a database
- Use the summary for quick review
- It might also integrate with vector stores to push text embeddings for search, etc.

### Status

As of late 2025 it's in preview. Expect evolving capabilities. In the exam context, know that Content Understanding exists to handle multimodal input and produce structured results using GenAI, and that it works within the Foundry ecosystem to complement search and document intelligence.

### When to Use

Use Content Understanding when you have a **variety of data types and need a unified processing pipeline**, or when you want AI-driven insight (like summaries, categorization) that goes beyond fixed-form extraction.

For example, processing a large set of research papers to both extract key findings and categorize them could be done with this single service rather than manually chaining cognitive skills.

## Integration Strategies

The knowledge mining and extraction tools can be combined. Often an architecture may use:
1. **Azure AI Content Understanding** or **Cognitive Search** with an index for initial processing and retrieval
2. **Azure OpenAI** to answer user queries (RAG) based on that content

For forms and specific documents:
- **Document Intelligence** handles the known structure extraction
- **Cognitive search** or **content understanding** tackle the unstructured parts (like long text)

**Always consider the nature of data:**
- If it's **highly structured**, use Form Recognizer
- If it's **unstructured** or a **mix**, consider Cognitive Search with skillsets or the new Content Understanding

And **ensure data governance** – these services output a lot of data, so manage where it's stored (indexes, knowledge store, etc.), especially if it includes sensitive info (use PII detection and perhaps redact or protect that data as needed).

[↑](#content)
