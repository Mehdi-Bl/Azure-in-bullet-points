# Implement Natural Language Processing (NLP) Solutions

## Text Analytics

### Key Phrase Extraction

Identifies the main points or "keywords" in text. For example, inputting a sentence "Azure Cognitive Services include Vision, Speech, Language, and Decision services" might extract key phrases like:
- "Azure Cognitive Services"
- "Vision"
- "Speech"
- "Language"
- "Decision services"

Useful for summarizing or tagging text.

### Entity Recognition

Finds **entities** in text and classifies them (e.g., people, organizations, locations, dates, etc.). Azure AI's entity recognition will output entities with types (e.g., "Microsoft" → Organization).

**PII Detection**: There is also PII detection which specifically finds personal identifiable info (phone numbers, emails, credit card numbers, etc.) and can mask them. This is useful for redacting sensitive info.

### Sentiment Analysis

Determines if text is **positive, negative, or neutral** in sentiment (and often returns a score 0–1 and possibly sentiment per sentence).

Use this on:
- Reviews
- Feedback
- Social media text, etc., to gauge tone

The AI can also provide **sentiment by aspect** if using Opinion Mining (e.g., "The camera is great but battery life is short" → positive about camera, negative about battery).

### Language Detection

Detects the language of the input text (returns language code and name, e.g., "en" for English). Useful in multi-lingual apps to route text to the correct translator or model.

## Language Detection and Translation

### Text Translation

**Azure AI Translator** can translate text between 100+ languages:
- Offers a simple REST API or SDK where you submit the text and target language
- Supports **document translation** (translating whole documents, preserving format via Azure Blob Storage)
- For real-time apps, the text API is typically used
- The translation service also supports **transliteration** (converting text from one script to another, e.g., Cyrillic to Latin)

It has features like:
- Automatic language detection
- You can also specify a **glossary** (user dictionary) to enforce certain translations of terms

## Custom Translator

If the standard translation isn't using your domain's terminology, you can build a **custom translation model**:

1. Use the **Custom Translator portal** to upload parallel documents (your own translations) to train a custom model that better respects your jargon
2. After training, **deploy** this custom model and call the Translator with a model identifier so it uses your custom model
3. For example, a medical company might train a custom model so that "CT" is always translated as "computed tomography" in full in another language rather than "courtesy telephone" or some wrong expansion

### Language Service

**Azure AI Language** (the unified service) often requires creating a Language resource and then calling various endpoints (for example, Text Analytics, Translator are part of it). Often these capabilities can be accessed through Azure's REST endpoints or the Azure Cognitive Service resource if you created a multi-service resource.

## Speech Services (STT, TTS, Speech Translation)

### Speech to Text (STT)

Azure AI Speech can convert **spoken audio to text**:
- Supports many languages
- Has specialized models (for conversation, dictation, or call center scenarios)
- Use the Speech SDK or REST API – you provide an audio stream or file and get back recognized text with timestamps
- Can also provide **NBest alternatives** and accuracy confidence

#### Customization

You can improve STT by using **Customization**:

**Custom Speech**: You can train a custom acoustic model or language model if you have domain-specific audio or vocabulary. For example, upload audio + human transcripts to adapt the model to new terminology (like recognizing chemical names).

**Phrase lists**: A simpler way to improve recognition of certain words (like product names) is providing a list of expected terms to the speech API at runtime to bias the recognition.

### Text to Speech (TTS)

Converts **text input to spoken audio**:
- Azure offers many **Neural voices** that are very natural (different languages, genders, styles)
- Choose a voice (e.g., en-US-JennyNeural) and optionally a style (cheerful, sad, etc. if supported)
- Send text, it returns audio (wav or ogg, etc.)
- You can adjust speaking speed, pitch, etc., using **Speech Synthesis Markup Language (SSML)**

#### SSML

SSML is an XML-based markup where you can specify:
- Pronunciations
- Breaks
- Emphasis

For example, wrap numbers in a say-as to spell them out or dictate them as a date.

Use SSML to fine-tune how the speech sounds, e.g., `<prosody rate="0.80">` to speak 20% slower, or `<phoneme>` to correct pronunciation.

#### Custom Voice

Azure allows creating a **Custom Neural Voice** (a synthesized voice that sounds like a specific person) but this requires a significant process (voice talent recording and Microsoft approval, primarily for accessibility or brand scenarios).

### Speech Translation

Combines STT and Translator – you input spoken audio in one language and get spoken output in another. The speech service does this by:
1. First transcribing
2. Translating
3. Then optionally using TTS for the target

The SDK can handle it end-to-end (you specify target languages and it produces either text or speech in the new language). This is useful for live interpretation scenarios.

### Intent Recognition in Speech

The speech service can integrate with **Language Understanding (LUIS/CLU)**. This is often called **Speech-to-Intent**.

Essentially, you can attach a Language Understanding model to the speech recognizer such that as it recognizes speech, it also parses the intent and entities. Alternatively, you first transcribe then send text to a language understanding endpoint. But a built-in integration allows direct speech->intent if using the Speech SDK with specified CLU credentials.

### Keyword Recognition

Azure Speech has a feature for **keyword spotting**:
- You can provide a keyword (like a wake word, e.g., "Hey Contoso")
- The SDK will continuously listen for just that keyword with low overhead
- When heard, you can trigger full speech recognition

This is used for wake-word detection scenarios. You train a keyword model or use some predefined ones.

### Diarization & Speaker ID

Advanced features include:
- **Speaker diarization**: When transcribing, label segments by speaker A vs speaker B in a conversation
- **Speaker Identification**: Recognizing who is speaking from a set of known speakers after an enrollment process

These can be used in meetings or call center transcripts. (Speaker identification requires creating voice profiles for individuals).

### Edge/Offline

Speech containers are available if you need offline. For example, a device can run a container for STT or TTS, using local computation but still requiring a license key.

When implementing, remember to handle audio input properly – e.g., use correct sampling rate (16k for voice usually), and the SDK simplifies microphone capture vs sending raw files. For TTS, handle the audio output (like playing it or saving to file).

## Conversational Language Understanding (LUIS / CLU)

### Intents, Utterances, Entities

A language understanding model's core:

#### Intent
The action or purpose behind a user's utterance. You define a set of intents for your application (e.g., "BookFlight", "CheckWeather", "None" for irrelevant).

#### Utterances
Example phrases that users might say to express each intent. You need to provide several example utterances per intent so the model learns to generalize.

For instance, for "BookFlight" intent, utterances like:
- "I need to book a flight to Paris next week"
- "Buy plane tickets"

#### Entities
Pieces of information in utterances that are important for fulfilling the intent. Entities could be:
- **Prebuilt types** like datetime, number, city
- **Custom ones** like "DestinationCity", "DepartureDate"

In our flight example, "Paris" might map to DestinationCity entity, "next week" to a DateRange entity.

### Training and Publishing

Using either LUIS (if it were still around) or the **Conversational Language Understanding** feature in Azure Language Service:
1. Label utterances with intents
2. Mark entities in them (LUIS portal or Language Studio allows you to highlight words to tag them as entities)
3. After labeling sufficient data, **train the model** which uses machine learning to create a language model
4. **Evaluate** its performance – check intent prediction accuracy and that entities are correctly extracted

### Iterate

If some utterances get wrong intent classification or entities missed, add those as new examples and retrain. Use a diverse set of phrasing (including different sentence structures, synonyms, etc.) to make the model robust.

### Testing

The portal provides a way to test by typing an utterance and seeing how the model classifies it and what entities are extracted. Use a set of test utterances that were not part of training to gauge real performance.

### Publish

Deploy the model to an endpoint (either the LUIS prediction endpoint or in Language Service, the project gets deployed as an endpoint in your resource).

You then call it via REST or SDK: send a query string, get back a JSON with:
- The top intent and its score
- Entity extractions (and their types, values)

Ensure you note the endpoint region and key, and any project identifiers when calling.

### Management

- **Backup** your model. LUIS allowed exporting the model as JSON, and CLU likely has similar (perhaps via CLI). This is important for versioning or moving between regions.
- Use **versioning** within a model to have a staging and production version.

### Optimization

If an intent is often confused with another, maybe they need clearer boundaries or more examples. Consider using **phrases lists** feature (in LUIS) to supply a list of words that are important for certain intents to help guide the model.

Also, watch out for having too many intents with not enough examples – that can degrade quality (the "None" intent should capture out-of-scope queries; always include it with various irrelevant examples).

### LUIS vs Conversational Language Understanding

Microsoft retired LUIS and moved to **CLU** under Azure Cognitive Service for Language. The concepts remain the same, but CLU uses the Language Studio. For the exam, just know the principles; if any difference, CLU supports a more modern transformer-based model and might require fewer examples or supports orchestration (like if you have multiple language apps that get called in a workflow).

## Question Answering with QnA

### Creating QnA Projects

In Language Studio (or formerly QnA Maker portal), you create a QnA knowledge base by:
- **Importing documents** (FAQs, manuals, web pages)
- OR **entering question-answer pairs** manually

The service will extract question-answer pairs from documents (for example, from an FAQ PDF it will try to pair questions with their answers).

### Adding QnA pairs

You can always add or edit questions and answers in the knowledge base. For each QnA pair, you can have:
- One answer
- Several possible question variations

#### Alternate Phrasings

These are different ways the same question might be asked. For instance:
- Original Q: "How do I reset my password?"
- Alternate: "Forgot my password, what to do?"

Providing these helps the service match user queries that aren't exact matches to the stored question text.

#### Metadata

You can tag QnAs with **metadata key-values** (like "product: windows" or "difficulty: easy"). At runtime, you can filter QnA queries by metadata (for example, if your bot knows the user is asking about Windows product, you can filter so it only returns QAs tagged with product=windows).

#### Multi-turn (Follow-up prompts)

You can link QnA pairs to enable a **multi-turn conversation**. For example, user asks "How to reset password?" the answer might be "Do you want to reset Windows password or Azure AD password?" – with two follow-up prompt options. Clicking one is a follow-up question leading to another QnA pair.

In QnA Maker, you set prompts on an answer that point to other QnA IDs. This creates a dialog flow capability within the QnA knowledge base.

### Training

QnA Maker/service uses the provided QnA pairs to build an embedding-based search index under the hood (and possibly some NLP). There isn't "training" you manually do (beyond adding QnAs); it continuously updates as you edit.

However, there is an **Active Learning** feature: when users ask questions that the service isn't confident about, it can suggest adding it or it can learn from what option the user picked. Essentially, it might show a couple of possible QnAs if confidence is low, and if the user picks one, that alternate phrasing can be added.

### Publishing & Endpoint

Once the KB is ready, **publish** it. This makes it available on an endpoint (in QnA Maker, the published KB had an endpoint key and ID; in Language service's Custom QnA, likely similar).

You then query it by calling the endpoint with a question (and optional metadata filters). The service returns the top answer(s) with a confidence score. It may also return a list of N best answers if you request, which you could use to disambiguate if needed.

### Multi-language QnA

You can create knowledge bases in different languages or a multi-lingual one. QnA Maker didn't automatically translate; you'd maintain separate QnA pairs per language or use a translator in front. The new service might allow multi-language in one project. Likely you handle by creating QnA sets in each language you want to support, or use built-in translation if provided when querying.

### Chit-Chat

You can import a **pre-built chit-chat dataset** to handle small talk. This is a quick way to make the bot friendly. The chit-chat data is just a collection of QnA pairs for conversational questions ("How are you?", "Tell me a joke", etc.) with answers in a chosen personality tone (professional, friendly, witty, etc.). You import those into your KB so that non-factual, casual questions get a proper answer.

### Testing

Use the integrated chat test to ask questions and see if the correct answers come. If not, adjust the phrasing or add the question. Often, slight rephrasing by users can stump the KB if not accounted for; try to preempt common variants.

### Export/Backup

Keep a backup of the QnA pairs (QnA Maker allowed .tsv or .json export). This also allows bulk editing in Excel for instance.

### Scaling & Limits

QnA KBs had limits like max 100k QnA pairs or so. Performance wise, the query is quite fast (uses Azure Search under the hood historically). If you need to handle very high QPS, consider deploying multiple QnA Service resources behind a load balancer or caching some results if appropriate (though caching QnA might be less needed due to quick responses).

### Alternative Approaches

With the rise of generative models, an alternative is using **Azure OpenAI with your data (RAG)** to answer questions from documents instead of a fixed QnA pair base. But the exam still covers QnA pair approach, which is useful when you want controlled, approved answers.

[↑](#content)
