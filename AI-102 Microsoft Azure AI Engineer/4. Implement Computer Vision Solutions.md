# Implement Computer Vision Solutions

## Azure AI Vision Overview

Azure AI Vision provides image analysis capabilities via **pre-trained models**:

### Visual Features

You can extract visual features like:
- **Categories**: Broad classifications of the image content
- **Tags**: Detailed keywords describing objects/scenes
- **Descriptions**: A sentence describing the image
- **Faces**: With bounding boxes and basic attributes like age/gender (if not disabled for privacy)
- **Objects**: Detected with coordinates

When selecting features, decide based on requirements:
- For a **photo moderation app**, you might request tags and adult content scores
- For an **app identifying products**, object detection is useful

### Object Detection & Tags

The service can:
- **Detect objects** in an image (e.g., "cat", "person", "car" with their locations)
- **Generate tags** which are single keywords for prominent things in the image

**Object detection** is useful for counting or locating specific items, while **tags** provide a quick summary of content.

### How to Call

Use the **Analyze Image API** (part of Azure AI Vision) and specify which features to return:
```
visualFeatures=Categories,Tags,Description,Objects
```

The response is JSON including the requested data.

**Interpretation**: For example, an image analysis response might tag an image with "outdoor", "tree", "dog" and describe it as "A dog jumping in the air to catch a frisbee".

As a developer, you must interpret **confidence scores** to decide if you trust the result (e.g., only act on tags above a certain confidence).

## Image Analysis and OCR

### OCR (Optical Character Recognition)

Azure AI Vision can extract text from images:

#### Printed Text
For printed text, the OCR capability (now often called "Read" in the API) reads text in images or PDFs. It supports multi-language detection.

#### Handwritten Text
For handwritten text, Azure AI Vision's OCR can also attempt to read cursive handwriting (with certain limitations on neatness).

These come as **lines and words with coordinates**. Use this to:
- Digitize documents
- Read text from photos (like signs, receipts in images)

### Spatial Analysis (Specialized)

Azure offers **Spatial Analysis** (primarily via the Video Analyzer for Media or Vision on IoT Edge) to detect:
- People's presence
- Movement
- Line crossing
- etc.

This is used for scenarios like counting people or detecting queue lengths. It runs as edge modules and might require specific setup – relevant if your solution involves real-time video in physical spaces.

### Best Practices

When analyzing images:
- Consider **resizing or compressing images** to within the service limits (most Vision API calls accept images up to 4MB, and certain resolution limits)
- Use **appropriate models** – Azure's Vision service has domain-specific models (e.g., for landmarks or celebrities recognition, or brands)
- If you need those, explicitly call those domain models

## Custom Vision (Classification and Detection)

For scenarios where the built-in tags/detection are insufficient (e.g., recognizing custom products, specific medical images), use **Custom Vision**:

### Image Classification vs. Object Detection

Decide the project type:

#### Classification
- Assigns **one or more labels to the entire image**
- Use this if your goal is to identify what type of image it is (e.g., classifying images of flowers into species)
- Can be **single-label** or **multi-label** (allowing multiple tags per image)

#### Object Detection
- **Detects instances of objects** within the image and labels each with a bounding box
- Use this when location in the image matters or multiple instances can appear (e.g., find all the instances of your company's logo in an image)

### Labeling Images

1. Upload a **representative set of images** to train on
2. You must **label them** with the target tags
3. For object detection, you **draw bounding boxes** around each object in the image and assign the appropriate label

**Good labeling is crucial** – ensure consistency (e.g., if one label is "apple" don't sometimes call it "fruit"; stick to a defined label set).

### Training the Model

Once images are labeled, you **train the model** in Custom Vision portal. The service will use those examples to learn. It outputs performance metrics like:
- **Precision**
- **Recall**
- **mAP** (mean average precision for detection)

#### Evaluate These Metrics

**Precision** is how accurate the predictions are (of those predicted "cat", how many were actually cat).

**Recall** is how complete the predictions are (of all the actual cats, how many did the model catch).

For imbalanced needs, you might prefer one metric over the other.

You can improve metrics by **adding more images**, especially for cases where the model performed poorly (check the failure cases in the portal).

### Iteration

Custom Vision supports **iterative training**:
- After an initial model, you can collect more images especially for cases it got wrong
- Label them, add to dataset, and retrain to improve
- The system can suggest misclassified images (if you use the prediction endpoint on a test set, you can see which it got wrong)

### Publishing & Consumption

Once satisfied, **publish the iteration** to an endpoint. This makes a REST endpoint available (with a prediction URL and key).

Consuming it involves:
- Sending the image (or image URL) to the endpoint
- Getting predicted labels (for classification) or bounding boxes with labels (for detection)
- You must include the **prediction-key** in the header

The returned JSON will include probabilities for each tag. Only the published iteration is callable; you can keep multiple iterations in development for comparison but one or more must be explicitly published.

### Custom Vision "Code First"

Instead of using the portal, you can use the **Custom Vision SDK/API** to upload images, tag them, and train programmatically. This is useful if you want to automate retraining (for example, periodically retrain with new data). The SDK allows:
- Creating a project
- Adding images with labels
- Training
- Getting the model

### Limits

Note Custom Vision has limits on:
- Number of projects
- Number of images
- Size of each image (typically <6MB and certain dimensions)

Also, training time grows with number of images; there are tier limits on computation (training hours).

### On-Device Use

If needed, you can **export Custom Vision models** for offline use on devices:
- Custom Vision supports exporting some models (classification models can often be exported as ONNX, TensorFlow, or even packaged in a Docker container)
- This is useful for edge scenarios (like a mobile app using the model locally)
- Object detection export might be limited to certain compact architectures

## Video Analysis with Azure AI Video Indexer

A service specialized for **extracting insights from videos**. It performs multiple analyses:

### Capabilities

- **Scene and shot detection** (identify scene changes)
- **Face detection and recognition** (even identify known people if you provide or enable a certain model – note: using face recognition might require uploading reference images or using celebrity recognition model)
- **Speech transcription** from the audio track, and identification of speakers (who spoke when, if possible)
- Extracts **keywords from speech**, and can detect **sentiment** in speech segments
- Detects **onscreen text** (OCR on video frames)

### Output

Video Indexer produces a **rich JSON** or can be browsed in its web portal, showing timeline of events (speech, faces, emotions, etc.). You can query this data or export it.

### Usage

Often via the Video Indexer portal or API:
1. Upload a video (or point to one in Azure Storage)
2. The service processes it
3. Query the index

It's useful for applications like:
- Media archives (to search inside videos)
- Content moderation
- Generating captions and highlights automatically

## Edge Deployment and Best Practices

### Spatial Analysis in Video

Azure AI Vision's **Spatial Analysis** (often run on IoT Edge) can analyze **live video feeds** (e.g., CCTV) to:
- Count people
- Detect movement in zones
- Detect occupancy

E.g., it can trigger an event if more than 5 people gather in a zone (for social distancing monitoring). This requires deploying the Spatial Analysis container and configuring the parameters (like zones, line crosses, etc.).

### Azure AI Vision for Video

There is also Live Video Analytics (though changes in services over time). But effectively, you can use Cognitive Services on video by **extracting frames periodically** and sending to Vision APIs.

For instance, for a custom scenario not fully covered by Video Indexer, you might grab 1 frame per second and call the image API or a custom model on each frame.

### Edge vs Cloud

- **Video Indexer** is cloud-based (although it has APIs that can be integrated)
- For **real-time low-latency needs**, consider edge-based analysis (using Cognitive Services containers for vision or custom models on the Edge)

### Responsible Use

If doing video analysis, be mindful of **privacy** (face recognition especially):
- Face API (face recognition) is now restricted – it requires application and was even deprecated for general use
- Ensure compliance with regulations if analyzing people in video

[↑](#content)
